---
title: "Brilliant_price_analysis"
author: "Дергунов Никита"
date: "2024-05-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(knitr)
library(kableExtra)
library(magrittr)
library(openxlsx)
library(ggplot2)
library(GGally)
library(ggpubr)
library(Hmisc)
library(corrplot)
library(ppcor)
library(readxl)
library(DiscriMiner)
library(pander)
library(DescTools)
library(EnvStats)
library(outliers)
library(robustHD)
library(lmtest)
library(sjPlot)
library(tseries)
library(leaps)
library(questionr)
library(outliers)
library(caret)
library(FactoMineR)
library(factoextra)
library(devtools)
library(rio)
library(psych)
library(ggpubr)
library(REdaS)
```

```{r, echo = TRUE}
getwd()
```

Подключим выбранный датасет:
```{r}
df <- read.xlsx('diamonds_prices.xlsx', sheet = 'Data')
```

Построим таблицу с описанием данных, обозначим зависимую и объясняющие переменные:
```{r, echo = FALSE, warning = FALSE, message = FALSE}
desc <- read.xlsx('diamonds_prices.xlsx', sheet = 'Description of data')
kbl(desc, caption = "Таблица 1. Описание данных", booktabs = T, 
    col.names = c("Переменная", "Описание переменной (анг)", "Описание переменной (рус)")) %>% 
  kable_classic_2(html_font = "Cambria", font_size = 12, full_width = F) %>%
  pack_rows("Зависимая переменная", 1, 1) %>%
  pack_rows("Объясняющие переменные", 2, 7)  %>%
  pack_rows("Прочее", 8, 11) 
```

Общую информацию о структуре датасета получим при использовании функции `str`:
```{r}
str(df)
```

## __Постановка задачи__

### __1. Описание и обоснование системы показателей, обоснование репрезентативности выборки:__

Для изучения мы выбрали набор данных о различных бриллиантах. В качестве исследуемой переменной мы решили взять параметр "price", то есть цену бриллианта, и посмотреть, как же она зависит от всех остальных показателей. 
Всего мы будем использовать 6 объясняющих переменных: 
1) вес бриллианта - переменная "carat", (в каратах)
2) общий процент глубины бриллианта - переменная "depth". Рассчитывается как глубина бриллианта, деленная на среднее значение длины и ширины, (в мм)
3) ширина вершины бриллианта относительно его максимальной ширины  - переменная "table", (в мм)
4) длина бриллианта - переменная "x", (в мм)
5) ширина бриллианта - переменная "y", (в мм)
6) глубина бриллианта - переменная "z", (в мм)
Остальные переменные не являются числовыми, поэтому рассматриваться в этом исследовании не будут. 

Данная выборка включает данные по более, чем 52000 (пятидесяти двум тысячам) различным бриллиантам, что является вполне достаточным количеством, чтобы выявить наличие различных закономерность или их отсутствие. Также мы считаем, что проведение данного исследование крайне полезно, поскольку это поможет выявить зависимость цены на бриллиант от его характеристик, что даст возможность предсказывать цены бриллиантов в дальнейшем. 

### __2. Выдвижение рабочих гипотез:__

Основные гипотезы нашего исследования заключаются в том, что мы хотим проверить, действительно ли стоимость бриллианта положительно зависит от его веса (переменной "carat") и размеров (переменные "x", "y", "z", "depth" и "table")

В процессе исследования также будут проверены:
- гипотеза о наличии аномальных наблюдений в выборке с помощью критерия Граббса 
- гипотеза о наличии аномальных наблюдений в выборке с помощью криетрия Рознера
- гипотеза о нормальном распределении совокупности с использованием статистического критерия Пирсона
- гипотеза о нормальном распределении совокупности с использованием статистического критерия Колмогорова-Смирнова
- гипотеза о значимости коэффициентов корреляции 
- гипотеза о значимости множественного коэффициента корреляции 
- гипотеза о нормальном распределении случайой ошибки в линейной регрессионной модели 
- гипотеза о нормальном распределении случайой ошибки в нелинейной регрессионной модели 


## __Основные характеристики СВ__
### __1. Характеристики положения СВ (среднее, мода, медиана):__

```{r, echo = FALSE}
paste('Среднее: ', round(mean(df$price), 3))
paste('Медиана: Me = ', median(df$price))
paste('Мода: Mo = ', Mode(df$price)) 
```

### __2. Характеристики разброса СВ (размах вариации, коэффициент вариации; дисперсия, стандартное отклонение):__

```{r, echo = FALSE}
paste('min = ', range(df$price)[1])
paste('max = ', range(df$price)[2])
paste('Размах: R = max - min = ', round(range(df$price)[2] - range(df$price)[1], 3))
paste('Коэффициент вариации: CV = ', round(CoefVar(df$price)*100, 3), '%')
paste('Дисперсия: Var(price) = ', round(var(df$price), 3))
paste('Стандартное отклонение: sd(price) = ', round(sd(df$price), 3))
```

### __3. Ранговые характеристики СВ:__

```{r, echo = FALSE}
pander(quantile(df$price)) 
```

### __4. Выводы:__

- Отметим, что значения среднего ($3931.12$) и медианы ($2401$) различаются более, чем в $1.5$ раз, что может служить предположением об отсутствии принадлежности к нормальному распределению исследуемой переменной. Наиболее частым значением цены является $605$, а сам факт наличия моды говорит нам о том, что не все наблюдения в выборке являются уникальными. Основными харатеристиками разброса переменной являются дисперсия и СКО со значениями $D \approx 15916932.15$ и $\sigma \approx 3989.6$, соответственно; Размах неочищенной от выбросов выборки равен $R = 18497$, что является очень большим значением, но в дальнейшем он будет меньше.

## __Диагностика выбросов__

### __1. Расчёт межквартильной разницы (IQR), применение правил 3σ и 1, 5IQR, 3IQR для диагностики нетипичных наблюдений:__

1) Межквартильная разница:
```{r}
pander(quantile(df$price)) 
paste('Межквартильная разница: IQR = Q3 - Q1 = ', IQR(df$price))
```

2) Правило 3σ:
```{r}
upper_bound <- mean(df$price) + 3 * sd(df$price)
lower_bound <- mean(df$price) - 3 * sd(df$price)
paste('Границы по правилу 3σ:', lower_bound, 'и', upper_bound)
out_of_3sigma <- boxplot.stats(df$price, coef = 2.438)$out
cat("Выбросы по правилу 3σ:", length(out_of_3sigma), "\n", sep=" ")
```

3) Правило 1,5 IQR:
```{r}
Q1 <- quantile(df$price, 0.25)
Q3 <- quantile(df$price, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
paste('Границы по правилу 1,5 IQR:', lower_bound, 'и', upper_bound)
out_of_1.5IQR <- boxplot.stats(df$price)$out
cat("Выбросы по правилу 1.5IQR:", length(out_of_1.5IQR), "\n\n", sep=" ")
```

4) Правило 3 IQR:
```{r}
Q1 <- quantile(df$price, 0.25)
Q3 <- quantile(df$price, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 3 * IQR
upper_bound <- Q3 + 3 * IQR
paste('Границы по правилу 3 IQR:', lower_bound, 'и', upper_bound)
price_sorted <- sort(df$price)
out_of_3IQR <- boxplot.stats(df$price, coef = 3)$out
cat("Выбросы по правилу 3IQR:", length(out_of_3IQR), "\n\n", sep=" ")
```

### __2. Построение ящичковых диаграмм:__

Выберем только числовые признаки:

```{r}
df_num <- df[, 2:8]
```

Построим ящичковые диаграммы для наших числовых признаков
```{r}
boxplot(df_num)
```

### __3. Проверка гипотезы о наличии аномальных наблюдений с помощью критерия Граббса/Рознера:__

Тест Граббса используется для проверки гипотезы $H_0$ о том, что одно максимальное или минимальное наблюдение в выборке является типичным наблюдением:
```{r}
grubbs.test(df_num$price)
grubbs.test(df_num$price, opposite = TRUE) 
```

Данный тест помог нам выявить, что максимальное и минимальное значения выборки являются аномальными. 

Тест Рознера, в отличие от Теста Граббса, может проверить больше одного наблюдения на предмет выбросов, возьмем k = 10 наблюдений:
```{r}
rosnerTest(df_num$price, k = 10)
```

Тест Рознера подтвердил наличие выбросов.

### __4. Выводы:__

Обратим внимание, что каждый тест показал нам различное количество выбросов, однако важно заметить, что правило $1.5IQR$ и тест Рознера (мы взяли только $10$ значений, но на самом деле их гораздо больше) показали примерно похожие значения. В свою очередь тест Граббса показал, что и наименьшее, и наибольшее значения являются выбросами. Из чего можно сделать вывод, что гипотеза и наличии аномальных наблюдейний подтвердилась на уровне значимости $0.05$. Поскольку правило 1,5IQR является наиболее точным и распространенным, мы решили ориентироваться именно на него при удалении выбросов.

Избавимся от выбросов по правилу 1,5 IQR:
```{r}
dim(df)
Q1 <- quantile(df$price, 0.25)
Q3 <- quantile(df$price, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
df_nooutliers <- subset(df, price >= lower_bound & price <= upper_bound)
dim(df_nooutliers)
```

## __Проверка соответствия эмпирического распределения нормальному закону__

### __1.  Расчёт коэффициентов асимметрии и эксцесса:__

Выведем коэффициенты асимметрии и эксцесса:
```{r, echo = FALSE}
paste('Коэффициент асимметрии: As = ', round(Skew(df_nooutliers$price), 3))
paste('Коэффициент эксцесса: Ek = ', round(Kurt(df_nooutliers$price), 3))
```

Заметим, что коэффициент асимметрии для нашей зависимой переменной положителен и значителен, что сведетельствует о сильном правостороннем скосе. Коэффициент эксцесса, в свою очередь, располагает значительной остроконечностью.

### __2. Построение гистограммы распределения, графика dotplot:__

Построим гистограмму распределения:
```{r, warning = FALSE, message = FALSE}
ggplot(df_nooutliers, aes(x = df_nooutliers$price)) + 
  geom_histogram(color = "black", fill = "lightblue") + 
  ylab("Относительная частота") +
  xlab("Цена в долларах США")
```

Построим гистограмму распределения нашей переменной `price` с наложением плотности нормального распределения:
```{r, warning = FALSE, message = FALSE}
ggplot(df_nooutliers, aes(x = df_nooutliers$price)) + 
  geom_histogram(aes(y=..density..), color = "black", fill = "lightblue") + 
  geom_line(aes(x = df_nooutliers$price, y = dnorm(df_nooutliers$price, mean = mean(df_nooutliers$price), sd = sd(df_nooutliers$price))), lwd = 1, col = 'red') +
  ylab("Относительная частота") +
  xlab("Цена в долларах США")
```

Заметим, что распределение цены на графике сильно отклоняется от нормального.

Построим график dotplot:
```{r}
#data <- df_nooutliers$price
#dotchart(data,
        # main = "dotplot",
         #xlab = "Цена в долларах США",
         #ylab = "Наблюдения",
        # color = "red"
#)
ggplot(df, aes(x = df_nooutliers$price, y = df$price)) +
  geom_dotplot(binaxis='y', stackdir='center', dotsize=1) +
  theme_minimal() +
  labs(title = "Dotplot", x = "Category", y = "Value")

```
### __3. Проверка гипотезы о нормальном распределении совокупности с использованием статистического критерия (критерий Пирсона, Колмогорова и т.д.):__

Для начала сформируем гипотезу:
\[
H_0 : \text{ выборка взята из генеральной совокупности с нормальным распределением }
\]

Критерий согласия Пирсона:
```{r}
PearsonTest(df_nooutliers$price)
```
Гипотеза о нормальности распределения рассматривамой нами переменной `price` отвергается на уровне значимости $\alpha = 0,05$, который рассматривается по умолчанию, однако стоит отметить, что гипотеза будет отвергаться на любых из общепринятых в использовании табличных уровнях значимости, так как значение p-value крайне мало. Исходя из результатов данного теста, можно смело сделать вывод, что рассматриваемая в нашем исследовании выборка *не* принадлежит нормальному закону распределения зависимой переменной.

Критерий Шапиро-Уилка (наиболее точный из-за Монте-Карло симуляций) требует меньше наблюдений (от 3 до 5000), однако распределение слишком сильно отличается от нормального, чтобы сокращать выборку практически в 11 раз.

Тест Колмогорова-Смирнова:
```{r}
pander(ks.test(df_nooutliers$price, 'pnorm', mean(df_nooutliers$price), sd(df_nooutliers$price)))
```

Гипотеза о нормальности по итогам проведения теста Колмогорова-Смирнова так же отвергается с вероятностью ошибки $\alpha = 0,05$ и, более того, на  любом из табличных уровней значимости, что так же подтверждает опровержение гипотезы принадлежности к нормальному распределению зависимой переменной `price`.

### __4. Выводы:__

- По итогам этого блока можем сделать вывод, что рассматриваемый набор данных не принадлежит нормальному распределению. Изначально по коэффициентам эксцесса и асимметрии мы выяснили, что график распределения имеет правосторонний скос, а выршина остроконечна. Оба эти фактора наводят на мысль об опровержении нормального закона распределения. После чего мы окончательно опровергли гипотезу об нормальном распределении переменной "price" с помощью тестов Пирсона и Колмогорова.

## __Корреляционный анализ__

### __1. Построение облака (поля) корреляции для исследования взаимосвязи между переменными:__

__До удаления выбросов:__

- Случаи положительной зависимости:
```{r}
ggscatter(df, x = "carat", y = "price", xlab = "Карат", ylab = "Цена в долларах США")
ggscatter(df, x = "x", y = "price", xlab = "Длина в мм", ylab = "Цена в долларах США")
ggscatter(df, x = "y", y = "price", xlab = "Глубина в мм", ylab = "Цена в долларах США")
ggscatter(df, x = "z", y = "price", xlab = "Высота в мм", ylab = "Цена в долларах США")
```

Каждая из объясняющих переменных, характеризующие длину, ширину и глубину бриллианта в миллиметрах, а так же его вес (в каратах), имеет положительную корреляционную зависимость с ценой бриллианта. Но важно отметить, что, судя по наклону, у параметра вес и длины эта зависимость слабая, а у ширины и глубины - очень слабая.

- Случаи отсутствия зависимости:
```{r}
ggscatter(df, x = "depth", y = "price", xlab = "Общий процент глубины", ylab = "Цена в долларах США")
ggscatter(df, x = "table", y = "price", xlab = "Ширина вершины относительно самой широкой точки", ylab = "Цена в долларах США")
```
Объясняющие переменные, характеризующие общий процент глубины и ширину вершины относительно самой широкой точки, имеют отсутствующую корреляционную связь между ними и зависимой переменной.

- Случаи с отрицательной зависимостью отсутствуют.

__После удаления выбросов:__

- Случаи положительной зависимости:
```{r}
ggscatter(df_nooutliers, x = "carat", y = "price", xlab = "Карат", ylab = "Цена в долларах США")
ggscatter(df_nooutliers, x = "x", y = "price", xlab = "Длина в мм", ylab = "Цена в долларах США")
ggscatter(df_nooutliers, x = "y", y = "price", xlab = "Глубина в мм", ylab = "Цена в долларах США")
ggscatter(df_nooutliers, x = "z", y = "price", xlab = "Высота в мм", ylab = "Цена в долларах США")
```

Для переменных "carat", "x", "y", "z" по-прежнему наблюдается слабая положительная зависимость между ними и зависимой пременной "price", изменения видны при сравнении графиков вплотную, ведь прогресс в виде более прямой зависимости после удаления выбросов имеется, пусть небольшой.

- Случаи отсутствия зависимости:
```{r}
ggscatter(df_nooutliers, x = "depth", y = "price", xlab = "Общий процент глубины", ylab = "Цена в долларах США")
ggscatter(df_nooutliers, x = "table", y = "price", xlab = "Ширина вершины относительно самой широкой точки", ylab = "Цена в долларах США")
```

Между переменными "depth" и "price", а так же между "table" и "price" корреляционная зависимость по-прежнему отсутствует.

- Случаи с отрицательной зависимостью отсутствуют.

### __2.1.  Построение и интерпретация матрицы парных коэффициентов корреляции и их сопоставление до и посел удаления выбросов:__ 

__До удаления выбросов:__
```{r}
cor_m1 <- cor(df_num, method = "pearson")
cor_m1[,1:7] 
```

Матрица парных коэффициентов корреляции до удаления выбросов показывает наибольшую корреляционную зависимость между объясняющими переменными "ширина", "длина" и "глубина", каждый из них попарно между собой, попарно с каратом, а так же попарно с ценой также положительно коррелируют. Так же одна из сильнейших зависимостей наблюдается между ценой за бриллиант и его весом. Наиболее низкая корреляционная зависимость имеется между показателем общего процента глубины и другими переменными, а так же между шириной вершины бриллианта относительно самой широкой точки и другими переменными. 
Таким образом, чем больше вес бриллианта, его длина, ширина и глубина, тем выше его цена. В это же время общий процент глубины бриллианта практически никак не влияет на цену, а ширина вершины относительно максимальной ширины влияет на цену очень слабо.

__После удаления выбросов:__
```{r}
df_nooutliers_num <- df_nooutliers[, 2:8]
cor_m1 <- cor(df_nooutliers_num, method = "pearson")
cor_m1[,1:7] 
```

Обратим внимание, что после удаления выбросов видимому изменению подверглась только корреляция между переменными "depth" и "price", но она все равно очень близка к 0, что также означает их независимость друг от друга. Поэтому получаем, что матрица парных коэффициентов корреляции после удаления выбросов кардинально не изменилась, все выводы касательно матрицы до удаления аномальных значений не подверглись изменению.

### __2.2. Проверка значимости коэффициентов корреляции до и после удаления выбросов:__

__До удаления выбросов:__

Комбинируем оценку корреляционной матрицы с результатами тестов на значимость:
```{r}
res <- cor.mtest(df_num, conf.level = 0.95)
corrplot(cor(df_num), p.mat = res$p, sig.level = 0.05, tl.col = "black", tl.srt = 45, tl.cex = 0.5) 
```

По итогу проверки на значимость парных коэффициентов корреляции до удаления выбросов можно утверждать, что все коэффициенты являются статистически значимыми.

__После удаления выбросов:__

Комбинируем оценку корреляционной матрицы с результатами тестов на значимость:
```{r}
res <- cor.mtest(df_nooutliers_num, conf.level = 0.95)
corrplot(cor(df_nooutliers_num), p.mat = res$p, sig.level = 0.05, tl.col = "black", tl.srt = 45, tl.cex = 0.5) 
```

По итогу проверки на значимость парных коэффициентов корреляции после удаления выбросов обнаружено, что корреляция между ценой и общим процентом глубины является статистически незначимой. Все остальные значения остаются статистически значимыми.

### __2.3. Выводы о взаимосвязи между признаками:__

Как мы заметили ранее, наибольшая положительная взаимосвязь у показателей длины, ширины и глубины между собой. Так же наблюдается высокая взаимосвязь между ценой за бриллиант и его длиной/шириной/глубиной, и еще сильнее между каратом и длиной/шириной/глубиной. Из чего можно сделать вывод, что габариты бриллианта крайне сильно влияют на его цену.

### __3. Построение доверительных интервалов для значимых коэффициентов корреляции:__

-

### __4.1. Построение матрицы частных коэффициентов корреляции до и после удаления выбросов с проверкой значимости:__

__До удаления выбросов:__

```{r}
cor_pb <- pcor(df_num)
cor_pb$estimate

print('P-VALUE')
cor_pb$p.value
table(cor_pb$p.value <= 0.05)
```

Можем заметить, что наиболее высокая корреляционная связь наблюдается между переменными, характеризующими цену и вес бриллианта, а так же между переменнымми, характеризующими вес и длину, что говорит о наиболее хорошей связи даже без влияния других признаков, однако показатели ширины и глубины бриллианта теперь располагают гораздо более низкой корреляционной связью с весом, из чего можно сделать противоположный вывод о том, что корреляционная зависимость между данными переменными сильна только тогда, когда на них оказывают влияние другие переменные.
То есть получается, что исключительно вес, а не форма в частном порядке положительно влияет на цену бриллианта.

Также заметим, что, в отличие от парных коэффициентов корреляции, статистически незначимых частных коэффициентов корреляции возросло до 4.

__После удаления выбросов:__

```{r}
cor_pa <- pcor(df_nooutliers_num)
cor_pa$estimate

print('P-VALUE')
cor_pa$p.value
table(cor_pa$p.value <= 0.05)
```
По результам после удаления выбросов коэффициенты изменились на очень маленькую величину. Направление связи между переменными не изменились. Однако теперь выборка располагает 2 статистически незначимыми частными коэффициентами корреляции, что, в целом, подтверждает, что убранные нами наблюдения были статистическими выбросами. 

### __4.2. Построение доверительных интервалов до и после удаления выбросво:__

__До удаления выбросов:__
```{r}
cor.mtest(cor_pb$e, conf.level = 0.05)
```

Результат так же показывает, что практически все частные коэффициенты корреляции являются значимыми. В доверительных интервалах переменных "х" и "table" и "y" и "z" все же есть $0$, поэтому это значения оказываются незначимыми.

__После удаления выбросов:__
```{r}
cor.mtest(cor_pa$e, conf.level = 0.05)
```

После удаления выбросов практически ничего не изменилось, зависимость претерпела малые изменения, поэтому можно сказать, как это мы видели выше, что большого влияния на характер взаимосвязей это не оказало. Возможно, дело в том, что рассматривается большое количество наблюдений.


### __5. Сравнение парных и частных коэффициентов корреляции, выводы о характере взаимосвязей:__

При сравнении парных и частных коэффициентов корреляции можно увидеть, что в парных взаимосвязях наблюдается высокая прямая корреляция между ценой и всеми габаритами бриллианта, в то время как при анализе частных коэф-ов связь между ценой и длиной, шириной, глубиной бриллианта практически отсутствует. Из этого следует, что отдельные характеристики предмета не влияют на стоимость, но это довольно понятно, поскольку бриллиант - объемный предмет, поэтому изменение одной характеристики в одномерном пространстве не может повлиять на что-либо, так как в реальной жизни при увеличении одной характеристики автоматически увеличиваются остальные. 
Также заметим, что при парной корреляции габариты бриллианта также сильно прямо связаны с его весом, в то время как при анализе частной взаимосвязи высокий коэф-нт вес имеет только с длиной.
Подводя итог, видно, что только совокупное действие факторов веса, длины, ширины и глубины бриллианта очень сильно влияют на его цену, в то время как по отдельности на цену со средней прямой связью влияет только вес, а остальные параметры при частном анализе влияния не оказывают.

### __6. Расчет множественного коэффициента корреляции и проверка значимости:__

Берем выборку без выбросов.

```{r}
cormatrix_nooutliers <- as.matrix(cor(df_nooutliers_num)) 
cormatrix_nooutliers 
```

```{r}
result <- c(1:ncol(df_nooutliers_num))
for (i in 1:ncol(df_nooutliers_num)){ 
  d <- det(cormatrix_nooutliers) 
  ac <- (-1) ^ (2 * i) * det(cormatrix_nooutliers[-i, -i]) 
  result[i] <- sqrt(1 - (d / ac)) 
} 
paste('Множественный коэффициент корреляции:')
result
```

Как мы можем заметить, преимущественно коэффициенты указывают на сильную связь между той или иной переменной и массивом остальных переменных. Практически все близки к нулю, за исключением двух переменных, характеризующих общий процент глубины и ширины вершины бриллианта относительной самой широкой точки, их корреляционная связь с включением остальных переменных является заметной и умеренной, соответственно.

F-статистики для переменных:
```{r}
a <- c()

for (i in result){
  s <- ((1/(7-1))*(i^2))/((1/(50397-7))*(1-(i^2)))  
  a <- append(a, s)
}

paste('Fнабл:', a)
```

Fкр(0.05; 6; 50390) = $2.09$
Fнабл(`price`) > Fкр
Fнабл(`carat`) > Fкр
Fнабл(`depth`) > Fкр
Fнабл(`table`) > Fкр
Fнабл(`x`) > Fкр
Fнабл(`y`) > Fкр
Fнабл(`z`) > Fкр

Все значения Fнабл > Fкр => гипотеза о независимости отвергается => все множественные коэффициенты корреляции являются значимыми.


## __Регрессионный анализ. Линейная регрессионная модель__

### __1. Построение линейных регрессионных моделей:__

Для начала построим двумерную модель линейной регрессии на основе нашей зависимой переменной и другой наиболее влиятельной для нее. Как мы могли заметить ранее, наибольшее внимание на цену бриллианта оказывает карат, то есть вес бриллианта.

На всякий случай удалим выбросы в переменной `carat`:
```{r}
df_new <- df_nooutliers_num[-which(df_nooutliers_num$carat %in% sort(df_nooutliers_num$carat)[(nrow(df_nooutliers_num) - 1):nrow(df_nooutliers_num)]),]
```

__Двумерная модель линейной регрессии:__
```{r}
lm1 <- lm(price ~ carat, data = df_new)
pander(summary(lm1))
```

Заметим, что $R^2$ = 0.84, что говорит о высокой объясняющей способности построенной нами модели. Видно, что переменные карат и цена обладают прямаой связью. Коэффициент перед признаком статистически значим.

Визуализируем полученные результаты:
```{r, warning = FALSE, message = FALSE}
ggplot(df_new, aes(y = price, x = carat)) + 
  geom_point() + 
  geom_smooth(method = 'lm') +
  xlab('Карат') +
  ylab('Цена в долларах США')
```

Теперь построим линейную модель множественной регрессии:

__Линейная модель множественной регрессии:__
```{r}
lm2 <- lm(price ~ ., data = df_nooutliers_num)
pander(summary(lm2))
```

Коэффициент детерминации все так же равен ~$0.84$, а все коэффициенты перед признаками так же статистически значимы, кроме переменной `z`(?)

### __2. Выбор лучшего линейного уравнения регрессии на основе анализа характеристик качества модели:__

Чтобы выявить наиболее лучшую и объясняющую модель, необходимо отобрать наиболее значимые регрессоры, которые, в свою очередь, оказывают наибольшее влияние на зависимую переменную, характеризующую цену за бриллиант. В этом нам помогут 2 метода: метод включения и метод исключения. При методе включения сначала строится модель с одним регрессором, для которого парный коэффициент корреляции с зависимой переменной является наибольшим по модулю. Далее добавляется следующий по тесноте линейной связи регрессор, и так до тех пор пока коэффициент при добавленном регрессоре не окажется незначимым. При методе исключения сначала строится полная модель со всеми регрессорами, далее исключается регрессор с наибольшим p-value и так до тех пор, пока не останутся регрессоры, которым соответствуют значимые коэффициенты.

__Метод включения:__
```{r, warning = FALSE, message = FALSE}
regfit_fwd <- regsubsets(price ~ ., df_nooutliers_num, intercept = TRUE, method = 'forward')

regfit_fwd_sum <- summary(regfit_fwd)
regfit_fwd_sum
names(regfit_fwd_sum)
```

Так как модели с разным числом регрессоров - сравниваем по скорректированному R2:
```{r}
regfit_fwd_sum$adjr2
```
Отобразим тенденцию включения на графике:
```{r}
plot(regfit_fwd_sum$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_fwd_sum$adjr2), regfit_fwd_sum$adjr2[which.max(regfit_fwd_sum$adjr2)],
       col = "red", cex = 2, pch = 20)
```

Результат данного метода показывает, что наибольшим коэффициентом детерминации, то есть наибольшей объясняющей способностью, обладает модель, включающая в себя $5$ объясняющих переменных, кроме переменной глубины бриллианта, так как она незначима

__Метод исключения:__
```{r, warning = FALSE, message = FALSE}
regfit_bwd <- regsubsets(price ~ ., df_nooutliers_num, intercept = TRUE, method = 'backward')

regfit_bwd_sum <- summary(regfit_bwd)
regfit_bwd_sum
names(regfit_bwd_sum)
```

Так как модели с разным числом регрессоров - сравниваем по скорректированному R2:
```{r, warning = FALSE, message = FALSE}
regfit_bwd <- regsubsets(price ~ ., df_nooutliers_num,  intercept = TRUE, method = 'backward')
regfit_bwd_sum <- summary(regfit_bwd)
regfit_bwd_sum$adjr2
```

Точно так же, как и в случае с методом включения, наилучшую объясняющую способность выдает модель с максимальным включением имеющихся регрессоров.

Отобразим тенденцию исключения на графике:
```{r}
plot(regfit_bwd_sum$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_bwd_sum$adjr2), regfit_bwd_sum$adjr2[which.max(regfit_bwd_sum$adjr2)],
       col = "red", cex = 2, pch = 20)
```

Анализируем остатки для линейной модели множественной регрессии.

Построим 3 графика для визуализации остатков:
```{r}
res <- lm2$residuals
plot(seq(1, nrow(df_nooutliers_num), 1), res, pch = 16, xlab = 'Номер наблюдения', ylab = 'Остатки')
abline(h = mean(res), col = 'red', lwd = 2)

hist(res, breaks = sqrt(length(res)), xlab = 'остаток', ylab = 'Частота',
     main = 'Гистограмма распределения частот остатков')

qqnorm(res, pch = 16, xlab = 'Теоретический квантиль', ylab = 'Выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res, col = 'red', lwd = 2)
```
Нормальный график квантиль-квантиль дает сделать вывод о том, что остатки распределены ненормально.

Так как тест Шапиро-Уилка не может быть проведен из-за объема выборки, воспользуемся тестом Жака-Бера:
```{r}
pander(jarque.bera.test(res))
```

Результаты используемого теста показывают, что нулевая гипотеза о нормальном распределении остатков нашей модели отвергается на уровне значимости 5% и , скорее всего, на любом рациональном уровне значимости.

### __3. Построение графика наблюдаемых и модельных значений зависимой переменной:__

Построим график наблюдаемых и модельных значений переменной, характеризующей цену бриллианта:
```{r}
df_sort <- df_nooutliers_num[order(df_nooutliers_num$price), ]
lm2_sort <- lm(price ~., df_sort)
pred <- predict(lm2_sort)
plot(seq(1, nrow(df_sort), 1), pred, col = "grey", pch = 20,
     xlab = 'Номер наблюдения', ylab = 'Цена в долларах США')
points(seq(1, nrow(df_sort), 1), df_sort$price, col = "red", pch = 20)
legend("topleft", legend=c("Модельная", "Фактическая"), col = c("grey", "red"), pch = 20)
```

### __4. Корректная запись уравнения регрессии и интерпретация всех коэффициентов и характеристик, включая коэффициенты эластичности:__

Оценки коэфф. регрессии:
```{r}
coef(lm2)
```

Уравнение линейной регрессии:
$y(`price`) = 11981.21464  + 7865.10875 x(`carat`) - 126.12313x(`depth`) - 72.84498x(`table`) - 789.07653x(`x`) + 345.64540x(`y`) - 19.16809x(`z`)$

Интервальные оценки коэффицентов регрессии:
```{r}
pander(coefci(lm2))
```

Коэффициенты эластичности:
```{r}
name <- c("carat", "depth", 'table', 'x', 'y', 'z')
e1 <- as.numeric(lm2$coefficients["carat"]*mean(df_nooutliers_num$carat)/mean(df_nooutliers_num$price))
e2 <- as.numeric(lm2$coefficients["depth"]*mean(df_nooutliers_num$depth)/mean(df_nooutliers_num$price))
e3 <- as.numeric(lm2$coefficients["table"]*mean(df_nooutliers_num$table)/mean(df_nooutliers_num$price))
e4 <- as.numeric(lm2$coefficients["x"]*mean(df_nooutliers_num$x)/mean(df_nooutliers_num$price))
e5 <- as.numeric(lm2$coefficients["y"]*mean(df_nooutliers_num$y)/mean(df_nooutliers_num$price))
e6 <- as.numeric(lm2$coefficients["z"]*mean(df_nooutliers_num$z)/mean(df_nooutliers_num$price))
elast_coef <- c(e1, e2, e3, e4, e5, e6)
df_e <- data.frame(name, elast_coef)
df_e
```

По полученным данным о коэффициентах эластичности можно увидеть, что параметры, характеризующие карат и ширину имеют положительное влияние на цену бриллианта, что нельзя сказать об общем проценте глубины, ширины вершины бриллианта относительно самой высококй точки, а также длины и глубины, поскольку те отрицательно влияют на нашу зависимую переменную. Предпосылки линейной модели не выполняются => нельзя утверждать, что данная модель является лучшей.

## **Регрессионный анализ. Нелинейная (степенная) регрессионная модель:**

Так как степенная нелинейная регрессионная модель строится на основе прологарифмированных признаков (как зависимого, так и объясняющих). Прологарифмируем наши переменные:

```{r}
df_nooutliers_num2 <- df_nooutliers_num
df_nooutliers_num2$price_log <- log(df_nooutliers_num$price)
df_nooutliers_num2$carat_log <- log(df_nooutliers_num$carat)
df_nooutliers_num2$depth_log <- log(df_nooutliers_num$depth)
df_nooutliers_num2$table_log <- log(df_nooutliers_num$table)
df_nooutliers_num2$x_log <- log(df_nooutliers_num$x)
df_nooutliers_num2$y_log <- log(df_nooutliers_num$y)
df_nooutliers_num2$z_log <- log(df_nooutliers_num$z)
df_nooutliers_num2_log <- df_nooutliers_num2[, 8:14]
```

### **1. Построение нелинейных (степенных) регрессионных моделей:**

Для начала построим двумерную модель степенной регрессии на основе нашей зависимой переменной и другой наиболее влиятельной для нее. Возьмем, как и в случае с линейной регрессией, параметр, характеризующий вес бриллианта:

На всякий случай избавимся от выбросов в переменной карат:

```{r}
df_new2 <- df_nooutliers_num2_log[-which(df_nooutliers_num2_log$carat_log %in% sort(df_nooutliers_num2_log$carat_log)[(nrow(df_nooutliers_num2_log) - 1):nrow(df_nooutliers_num2_log)]),]
```

**Двумерная модель степенной регрессии:**

```{r}
sm3 <- lm(price_log ~ carat_log, data = df_new2)
pander(summary(sm3))
```

Заметим, что $R^2$ = 0.92, что говорит о высокой объясняющей способности построенной нами степенной модели. Видно, что переменные карат и цена обладают прямой связью. Коэффициент перед признаком статистически значим.

Визуализируем полученные результаты:

```{r, warning = FALSE, message = FALSE}
ggplot(df_new2, aes(y = price_log, x = carat_log)) + 
  geom_point() + 
  geom_smooth(method = 'lm') +
  xlab('Карат') +
  ylab('Цена в долларах США')
```

Теперь построим степенную модель множественной регрессии:

**Степенная модель множественной регрессии:**

```{r}
sm4 <- lm(price_log ~ ., data = df_nooutliers_num2_log)
pander(summary(sm4))
```

### **2. Выбор лучшего линейного уравнения регрессии на основе анализа характеристик качества модели:**

------------------------------------------------------------------------

**Метод включения:**

```{r, warning = FALSE, message = FALSE}
regfit_fwd2 <- regsubsets(price_log ~ ., df_nooutliers_num2_log, intercept = TRUE, method = 'forward')

regfit_fwd_sum2 <- summary(regfit_fwd2)
regfit_fwd_sum2
names(regfit_fwd_sum2)
```

Так как модели с разным числом регрессоров - сравниваем по скорректированному R2:

```{r}
regfit_fwd_sum2$adjr2
```

Отобразим тенденцию включения на графике:

```{r}
plot(regfit_fwd_sum2$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_fwd_sum2$adjr2), regfit_fwd_sum2$adjr2[which.max(regfit_fwd_sum2$adjr2)],
       col = "blue", cex = 2, pch = 20)
```

Результат данного метода показывает, что наибольшим коэффициентом детерминации, то есть наибольшей объясняющей способностью, обладает модель, включающая в себя $5$ объясняющих переменных, кроме переменной глубины бриллианта, так как она незначима

**Метод исключения:**

```{r, warning = FALSE, message = FALSE}
regfit_bwd2 <- regsubsets(price_log ~ ., df_nooutliers_num2_log, intercept = TRUE, method = 'backward')

regfit_bwd_sum2 <- summary(regfit_bwd2)
regfit_bwd_sum2
names(regfit_bwd_sum2)
```

Так как модели с разным числом регрессоров - сравниваем по скорректированному R2:

```{r, warning = FALSE, message = FALSE}
regfit_bwd2 <- regsubsets(price_log ~ ., df_nooutliers_num2_log,  intercept = TRUE, method = 'backward')
regfit_bwd_sum2 <- summary(regfit_bwd2)
regfit_bwd_sum2$adjr2
```

Точно так же, как и в случае с методом включения, наилучшую объясняющую способность выдает степенная модель с максимальным включением имеющихся регрессоров.

Отобразим тенденцию исключения на графике:

```{r}
plot(regfit_bwd_sum2$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_bwd_sum2$adjr2), regfit_bwd_sum2$adjr2[which.max(regfit_bwd_sum2$adjr2)],
       col = "blue", cex = 2, pch = 20)
```

Обратим внимание, что в это методе результат не изменился
Построим 3 графика для визуализации остатков:

```{r}
res2 <- sm4$residuals
plot(seq(1, nrow(df_nooutliers_num2_log), 1), res2, pch = 16, xlab = 'Номер наблюдения', ylab = 'Остатки')
abline(h = mean(res2), col = 'blue', lwd = 2)

hist(res2, breaks = sqrt(length(res2)), xlab = 'Остаток', ylab = 'Частота',
     main = 'Гистограмма распределения частот остатков')

qqnorm(res2, pch = 16, xlab = 'Теоретический квантиль', ylab = 'Выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res2, col = 'blue', lwd = 2)
```

Нормальный график квантиль-квантиль дает сделать вывод о том, что остатки распределены нормально, чего нельзя сказать про линейную модель множественной регрессии.

Так как тест Шапиро-Уилка не может быть проведен из-за объема выборки, воспользуемся тестом Жака-Бера:
```{r}
pander(jarque.bera.test(res2))
```

Результаты используемого теста показывают, что нулевая гипотеза о нормальном распределении остатков нелинейной (степенной) модели отвергается на уровне значимости 5% и , скорее всего, на любом рациональном уровне значимости.
### __3. Построение графика наблюдаемых и модельных значений зависимой переменной:__

Построим график наблюдаемых и модельных значений переменной, характеризующей цену бриллианта:
```{r}
df_sort2 <- df_nooutliers_num2_log[order(df_nooutliers_num2_log$price_log), ]
sm4_sort <- lm(price_log ~., df_sort2)
pred <- predict(lm2_sort)
plot(seq(1, nrow(df_sort2), 1), pred, col = "grey", pch = 20,
     xlab = 'Номер наблюдения', ylab = 'Цена в долларах США')
points(seq(1, nrow(df_sort2), 1), df_sort2$price_log, col = "blue", pch = 20)
legend("topleft", legend=c("Модельная", "Фактическая"), col = c("grey", "blue"), pch = 20)
```

### __4. Корректная запись уравнения регрессии и интерпретация всех коэффициентов и характеристик, включая коэффициенты эластичности:__

Оценки коэфф. регрессии:
```{r}
coef(sm4)
```

Уравнение нелинейной (степенной) регрессии:
y(`price`) = 10.7261 + 0.9703x(`carat_log`) - 0.6774x(`depth_log`) - 0.8399x(`table_log`) + 1.2647x(`x_log`) + 0.8180x(`y_log`) + 0.0235x(`z_log`)

Интервальные оценки коэффицентов регрессии:
```{r}
pander(coefci(sm4))
```

```{r}
name <- c("carat_log", "depth_log", 'table_log', 'x_log', 'y_log', 'z_log')
E1 <- as.numeric(sm4$coefficients["carat_log"]*mean(df_nooutliers_num2_log$carat_log)/mean(df_nooutliers_num2_log$price_log))
E2 <- as.numeric(sm4$coefficients["depth_log"]*mean(df_nooutliers_num2_log$depth_log)/mean(df_nooutliers_num2_log$price_log))
E3 <- as.numeric(sm4$coefficients["table_log"]*mean(df_nooutliers_num2_log$table_log)/mean(df_nooutliers_num2_log$price_log))
E4 <- as.numeric(sm4$coefficients["x_log"]*mean(df_nooutliers_num2_log$x_log)/mean(df_nooutliers_num2_log$price_log))
E5 <- as.numeric(sm4$coefficients["y_log"]*mean(df_nooutliers_num2_log$y_log)/mean(df_nooutliers_num2_log$price_log))
E6 <- as.numeric(sm4$coefficients["z_log"]*mean(df_nooutliers_num2_log$z_log)/mean(df_nooutliers_num2_log$price_log))
Elast_coef <- c(E1, E2, E3, E4, E5, E6)
df_E <- data.frame(name, Elast_coef)
df_E
```


По полученным данным о коэффициентах эластичности можно увидеть, что параметры, характеризующие длину, ширину и глубину имеют положительное влияние на цену бриллианта, что нельзя сказать об общем проценте глубины, ширины вершины бриллианта относительно самой высококй точки, а также карат, поскольку те отрицательно влияют на нашу зависимую переменную. 


## **Регрессионный анализ. Итог**

```{r}
IC_table <- data.frame(n = c('lm2', 'sm4'),
                       a = c(AIC(lm2), AIC(sm4)),
                       b = c(BIC(lm2), BIC(sm4)))
kbl(IC_table,
    caption = "Таблица 2. Информационные критерии Акаике и Шварца", 
    booktabs = T, col.names = c("Model", "AIC", "BIC")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

**Вывод:** поскольку значения AIC и BIC наименьшие у модели (sm4), а значение R^2 больше, то это говорит нам о том, что нелинейная (степенная) модель лучше объясняет нашу зависимую переменную.


```{r}
sum(is.na(df))
```

Выберем только числовые признаки:

```{r}
df_num <- df[, 3:8]
df_new <- scale(df_num)
```

Предварительный визуальный анализ распределений:

```{r}
boxplot(df_num, xaxt = "n", cex.lab = 0.7)
text(x =  seq_along(names(df_new)), y = par("usr")[3] - 1, srt = 35, adj = 1,
     labels = names(df_num), xpd = TRUE, cex = 0.6)
```

Снижение размерности будет достигаться, если признаки, описывающие совокупность, достаточно коррелированы между собой.

```{r}
corrplot(cor(df_new), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

Применим тест сферичности Бартлетта, который используется для оценки эффективности методов снижения размерности. Данный тест проверяет нулевую гипотезу о том, что теоретическая корреляционная матрица многомерного распределения вектора случайных величин, является единичной матрицей. Если нулевая гипотеза отвергается, предпосылки применения метода главных компонента будут выполнены.

```{r}
bart_spher(df_new)
```

Значение p-value проведенного теста меньше уровня значимости 5%. Нулевая гипотеза отвергается, применение МГК обосновано.

Перейдем к самому методу главных компонент. Определим главные компоненты:

```{r}
pc <- PCA(df_new, graph = FALSE)
```

Собственные значения и доля суммарной вариации исходного набора признаков, приходящаяся на главные компоненты:

```{r}
pander(pc$eig)
```

Сделаем тоже самое, но немного другим способом

```{r}
princomp <- princomp(df_new, cor = TRUE) # если не указать cor, то будет использоваться ковариационная матрица
summary(princomp)
```

```{r}
pander(princomp$loadings[,1:3])
corrplot(princomp$loadings[,1:6], is.corr = FALSE, tl.col = "black")
```

Проверим, что главные компоненты линейно независимы:

```{r}
cor.test(princomp$scores[,3], princomp$scores[,2])
cor.test(princomp$scores[,3], princomp$scores[,4])
cor.test(princomp$scores[,3], princomp$scores[,6])

```

p-value = 1, ГК линейно независимы (справедливо для всех компонент, для примера оставили 3).

## **Выделение главных компонент:**

1)  Для начала рассмотрим метод Кайзера:

```{r}
pander(pc$eig)

plot(eigen(cor(df_new))$values, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Собственное значение')
abline(h = 1, col = 'red', lwd = 2)
text(x = 13, y = 1.2, 'eigen value = 1', col = 'red')
```

Выберем те главные компоненты, которым соответствует собственное значение \> 1 (по методу Кайзера). Следуя из этого, в качестве ГК в анализе используем 2 первые компоненты.

2)  Доля суммарной вариации:

В данном методе оптимальное число ГК определяется в соответствии с долей суммарной вариации исходных признаков, которую требуют сохранить (принято брать 70-80%).

```{r message=FALSE, warning=FALSE}
cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')
abline(h = 80, col = 'red', lwd = 2)
text(x = 13, y = 85, '80%', col = 'red')
```

Метод показывает, что в наш интервал попадает только 1 главная компонента.

3)  Критерий каменистой осыпи:

В данном методе мы на основе графика определяем, на каком числе компонент происходит резкий спад доли сохраненной дисперсии.

```{r}
fviz_eig(pc, addlabels = TRUE)
```

Наблюдаем резкое понижение с 65.7 до 21.4 на интервале 1-2, следовательно выбираем 2 главные компоненты.

Вывод: на основании 3 методов делаем вывод, что целесообразнее делить на 2 компоненты. В последнем пункте определим как стоит поделить наши признаки по компонентам.

### **2. Описание суммарного вклада первых главных компонент**

Вклады переменных в процентах в каждую компоненту:

```{r}
fviz_contrib(pc, choice = "var", axes = 1)
```

```{r}
fviz_contrib(pc, choice = "var", axes = 2)
```

Из графиков видно, что наибольший вклад в 1 ГК у переменных: "z, x, y, carat". Во 2 ГК ниабольший вклад у переменных: "depth, table". Отсюда выявляется очевидная гипотеза деления на 2 компоненты. Далее проведем корреляционнный анализ для подтверждения выдвинутой гипотезы.

### **3. Построение графика накопленного вклада главных компонент в суммарную дисперсию исходного признакового пространства**

```{r}
pander(pc$eig)
```

Согласно таблице первые 2 компоненты учитывают 87.12% дисперсии. Первая компонента объясняет 65.72% дисперсии, а вторая 21.4%.

Теперь можем построить график накопленной дисперсии.

```{r  message=FALSE, warning=FALSE}
cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')
```

График показывает, что после 2 компоненты линия кривая становится более пологой, следовательно вклад всех последующих ГК будет всё меньше и меньше.

### **4. Интерпретация главных компонент на основе анализа матрицы факторных нагрузок. Дать названия выделенным главным компонентам:**

```{r  message=FALSE, warning=FALSE}
pca <- principal(cor(df_new), nfactors = 2, rotate = "none", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 2, nrow = ncol(df_num))
rownames(pca_loadings) <- colnames(df_new)

kbl(round(pca_loadings, 3),
    caption = "Таблица 1. Матрица факторных нагрузок", 
    booktabs = T, col.names = c("Factors", "PC1", "PC2")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Факторные нагрузки показывают насколько сильная корреляция между переменными и компонентой. Чем больше связь между переменным и компоненотой, к той компонентой и относится переменная. Но можно в некоторых случаях просто логически из одной компоненты перенести в другую, как сделали мы c признаком "table", при этом сохранив смысл и учтя матрицу факторных нагрузок. Видим слудещее деление: в 1 ГК "x, y, z, carat", а в 2 ГК "table, depth".

Далее используем варимакс вращение:

```{r  message=FALSE, warning=FALSE}
pca_vm <- principal(cor(df_new), nfactors = 2, rotate = "varimax", covar = FALSE)

pca_loadings_vm <- matrix(as.numeric(pca_vm$loadings), ncol = 2, nrow = ncol(df_num))
rownames(pca_loadings_vm) <- colnames(df_new)

kbl(round(pca_loadings_vm, 3),
    caption = "Таблица 1. Матрица факторных нагрузок", 
    booktabs = T, col.names = c("Factors", "PC1", "PC2")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Вращение не поменяло распределение переменных по двум ГК (поменялись "table" и "depth" местами, но так как мы их располагаем в одной ГК, то ничего не изменилось).

Дадим название нашим компонентам:

1 - "Основные параметры алмазов" 2 - "Геометрические параметры огранки"

## **Построение уравнения регрессии с использованием выделенных ГК:**

Считываем данные и строим прошлые наилучшие модели - линейную и нелинейную.

```{r}
Q1 <- quantile(df$price, 0.25)
Q3 <- quantile(df$price, 0.75)
IQR <- Q3 - Q1             
lower_bound <- Q1 - 1.5 * IQR 
upper_bound <- Q3 + 1.5 * IQR 
df_nooutliers <- subset(df, price >= lower_bound & price <= upper_bound)
df_nooutliers_num <- df_nooutliers[, 2:8]
```

ЛИНЕЙНАЯ МОДЕЛЬ
```{r}
lm2 <- lm(price ~ ., data = df_nooutliers_num)
pander(summary(lm2))
```
НЕЛИН м
```{r}
df_nooutliers_num2 <- df_nooutliers_num
df_nooutliers_num2$price_log <- log(df_nooutliers_num$price)
df_nooutliers_num2$carat_log <- log(df_nooutliers_num$carat)
df_nooutliers_num2$depth_log <- log(df_nooutliers_num$depth)
df_nooutliers_num2$table_log <- log(df_nooutliers_num$table)
df_nooutliers_num2$x_log <- log(df_nooutliers_num$x)
df_nooutliers_num2$y_log <- log(df_nooutliers_num$y)
df_nooutliers_num2$z_log <- log(df_nooutliers_num$z)
df_nooutliers_num2_log <- df_nooutliers_num2[, 8:14]
sm4 <- lm(price_log ~ ., data = df_nooutliers_num2_log)
pander(summary(sm4))
```

Теперь составим уравнение регрессии по главным компонентам
```{r}
pca <- as.data.frame(pc$ind$coord[, 1:2])
pc1 <- pca$Dim.1
pc2 <- pca$Dim.2
lm_pca <- lm(df$price ~ pc1 + pc2)
pander(summary(lm_pca))
```

В построенной регрессионной модели на главные компоненты, все коэффициенты в модели значимы на уровне 5%. В данной модели выполнена предпосылка об отсутствии мультиколлинеарности между объясняющими переменными.


```{r}
IC_table <- data.frame(n = c('lm_pca', 'lm2', 'sm4'),
                       a = c(AIC(lm_pca), AIC(lm2), AIC(sm4)),
                       b = c(BIC(lm_pca), BIC(lm2), BIC(sm4)))
kbl(IC_table,
    caption = "Таблица 1. Информационные критерии Акаике и Шварца (Баесовский инф. критерий)", 
    booktabs = T, col.names = c("Модель", "Значение AIC", "Значение BIC")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Видно, что коэф R^2 для модели sm4 наивысший, что говорит нам о том, что эта модель является самой точной для анализа наших данных

## **Кластерный анализ:**

Так как наш датасет имеет свыше $50 000$ наблюдений, проводить кластерный анализ на всех них не является рациональным, поэтому сократим выборку до $500$ наблюдений. Обычно иерархические процедуры оказываются эффективными для выборок сравнительно скромного объема. Будем работать исключительно с объясняющими переменными, поэтому цена за бриллиант не будет учтена при проведении кластерного анализа.

```{r}
df1 <- df[1:500, c('carat', 'depth', 'table', 'x', 'y', 'z')]
str(df1)
```

Для дальнейшего проведения кластерного анализа необходимо стандартизировать наши признаки, так как данная процедура является обязательной для применения Евклидовой метрики.

Отберем классфицирующие признаки:

```{r}
corrplot(cor(scale(df1)), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

Уже можем заметить, что визуально множество объясняющих переменных образуют мультиколлинеарность, убедимся в этом ниже, исключим такие переменные:

```{r}
pander(cor.mtest(df1)$p)
```

Как мы можем заметить, сильная корреляция наблюдается между переменными, характеризующими карат, длину, ширину и глубину, что не является допустимым для дальнейшего анализа. Остальные нам подойдут, но так как общего процента глубины и ширины вершины бриллианта относительно самой широкой точки нам будет мало, из группы переменных, которые коррелируют друг с другом, исключим все, кроме карата.

```{r}
df2 <- df1[, -c(4, 5, 6)]
str(df2)
```

```{r}
cluster_df <- as.data.frame(scale(df2))
```

Построим матрицу расстояний в Евклидовом пространстве. Даже при сокращении выборка будет большой, построим для случайной выборки из 30 наблюдений:

```{r}
eucl_dist <- dist(cluster_df[sample(rownames(cluster_df), 30),], method = 'euclidian')
fviz_dist(eucl_dist)
```

Изначально предположим, что кластеров столько, сколько переменных, так как у нас маленькое количество переменных, а именно 3. Так как в кластерном анализе гиперпараметром является число кластеров, это означает, что до рассмотрения вариантов разбиения объектов на кластеры, мы должны будем определить оптимальное число кластеров.

В определении количества кластеров нам помогут такие методы, как: **Метод локтя** и **Метод силуэтов**. Чтобы убедиться в результатах, нам так же не помешает **Статистка разрыва**.

Правило выбора числа кластеров, основанное на WSS, называется иногда **Методом локтя** (elbow method). Принято считать оптимальным число кластеров, после которого убывание WSS начинает замедляться, т.н. точка изгиба, отсюда название.

```{r}
fviz_nbclust(cluster_df, kmeans, method = 'wss') +
  labs(x = 'Число кластеров', y = 'Сумма внутрикластерных дисперсий',
       title = 'Зависимость WSS от числа кластеров')
```

Заметим, что данный метод предлагает 3 кластера (наклон сильно меняется после значения, равного 3), что подтвеждает наше предположение, обратимся к следующему методу.

Воспользуемся **Методом силуэтов**. При кластеризации хорошего качества среднее расстояние от объекта до соседей по кластеру не должно превышать минимального среднего расстояния по прочим кластерам. Оптимальным следует считать значение параметра, соответствующее наибольшему среднему значению ширины силуэта.

```{r}
fviz_nbclust(cluster_df, kmeans, method = 'silhouette') +
  labs(x = 'Число кластеров', y = 'Средняя ширина силуэта по всем точкам',
       title = 'Зависимость средней ширины силуэта от числа кластеров')
```

Данный метод так же указывает 3 кластера в качестве оптимального количества.

Воспользуемся **Статистикой разрыва**. Нулевая гипотеза состоит в том, что выборка взята из однородной (объекты не кластеризуемы) генеральной совокупности с некоторым параметризуемым распределением. Идея: генерируются $B$ выборок, проводится кластерный анализ, вычисляется $WSS^*_B$.

```{r}
fviz_nbclust(cluster_df, kmeans, method = 'gap_stat') +
  labs(x = 'Число кластеров', y = 'Статистика разрыва',
       title = 'Зависимость статистики разрыва от числа кластеров')
```

Результат, в отличие от методов, указывает на 1 кластер. Будем считать его недействительным

Итак, оптимальным количеством кластеров будем считать 3. Определились.

Теперь рассмотрим разные варианты разбиения объектов на кластеры. Проведем иерархическую кластеризацию с количеством кластеров равным 3.

1)  **Метод Варда** (как правило, дает наиболее удачное разбиение, основан на минимизации суммы внутрикластерных дисперсий):

```{r}
hclust_w <- hcut(cluster_df, k = 3, hc_metric = 'euclidian', hc_method = 'ward.D2')
```

2)  **Метод ближнего соседа:**

```{r}
hclust_nn <- hcut(cluster_df, k = 3, hc_metric = 'euclidian', hc_method = 'single')
```

3)  **Метод дальнего соседа:**

```{r}
hclust_fn <- hcut(cluster_df, k = 3, hc_metric = 'euclidian', hc_method = 'complete')
```

4)  **Метод средней связи:**

```{r}
hclust_av <- hcut(cluster_df, k = 3, hc_metric = 'euclidian', hc_method = 'average')
```

5)  **Метод центра тяжести:**

```{r}
hclust_c <- hcut(cluster_df, k = 3, hc_metric = 'euclidian', hc_method = 'centroid')
```

Визуализируем результаты для каждого метода:

1)  **Метод Варда:**

```{r, warning = FALSE, message = FALSE}
hclust_w$labels <- df$Сountry
fviz_dend(hclust_w,
          cex = 0.5, # размер подписей
          color_labels_by_k = TRUE, # выделить объекты цветом по принадлежности к кластерам
          main = 'Дендрограмма (принцип Варда)', ylab = 'Расстояние')
```

2)  **Метод ближнего соседа:**

```{r, warning = FALSE, message = FALSE}
hclust_nn$labels <- df$Сountry
fviz_dend(hclust_nn, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип ближнего соседа)', ylab = 'Расстояние')
```

3)  **Метод дальнего соседа:**

```{r, warning = FALSE, message = FALSE}
hclust_fn$labels <- df$Сountry
fviz_dend(hclust_fn, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип дальнего соседа)', ylab = 'Расстояние')
```

4)  **Метод средней связи:**

```{r, warning = FALSE, message = FALSE}
hclust_av$labels <- df$Сountry
fviz_dend(hclust_av, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип средней связи)', ylab = 'Расстояние')
```

5)  **Метод центра тяжести:**

```{r, warning = FALSE, message = FALSE}
hclust_c$labels <- df$Сountry
fviz_dend(hclust_c, cex = 0.5, color_labels_by_k = TRUE,
          main = 'Дендрограмма (принцип центра тяжести)', ylab = 'Расстояние')
```

Заметим, что первый метод, а именно **Метод Варда**, показывает лучшие результаты разбиения наблюдений на кластеры, неплохо справляется и **Метод дальнего соседа**.

## **Использование метода к-средних для классификации объектов:**

Используем для **Метода K-means** 3 кластера, как и утвердили ранее.

```{r}
kmeans3 <- kmeans(cluster_df, centers = 3)
kmeans3
```

```{r}
kmeans3$centers[1,]
```

### **1. Построение и анализ графика средних значений показателей в кластерах:**

Теперь визуализируем результаты кластеризации через график средних. По графику даем интерпретацию полученным кластерам.

```{r}
plot(1:ncol(cluster_df), kmeans3$centers[1,], xaxt = 'n', type = 'l', col = 'blueviolet', 
     cex.sub = 0.3, cex.lab = 0.8, lwd = 2, ylim = c(-2, 5), 
     ylab = 'Среднее значение признака', xlab = 'Классифицирующий признак')

lines(1:ncol(cluster_df), kmeans3$centers[2,], type = 'l', col = 'firebrick3', lwd = 2)
lines(1:ncol(cluster_df), kmeans3$centers[3,], type = 'l', col = 'orange', lwd = 2)

title('График средних (признаки стандартизованы)')
legend(0.8, 5, c('Кластер 1', 'Кластер 2', 'Кластер 3'),
       lwd = c(2, 2, 2, 2), col = c('blueviolet', 'firebrick3', 'orange'))
text(x =  seq_along(names(df2)), y = par("usr")[3] - 1, srt = 15, adj = 0.5,
     labels = names(df2), xpd = TRUE, cex = 0.5)
```

Можем заметить, что некоторые кластеры пересекаются друг с другом. Данный график не дает нам понять, какую долю занимают наблюдения при том или ином параметре, мы рассмотрим это чуть позже. Вместе с тем охарактеризуем выведенные кластеры.

### **2. Проверка гипотезы о равенстве средних:**

Проверим гипотезу о равенстве средних:

```{r}
cluster_df$Cluster <- kmeans3$cluster
columns <- setdiff(names(cluster_df), "Cluster")
for (i in columns) {
 column <- cluster_df[, i]
 anova_test <- aov(column ~ Cluster, data = cluster_df)
 print(i)
 print(summary(anova_test))
}
```

Видим, что каждая переменная демонстрирует статистически значимое различие между средними, что означает, что гипотеза о равенстве средних отвергается для каждой из трех переменных. Распределение наблюдений на 3 кластера, как мы выявили ранее, будет оптимальным.

### **3. Интерпретация полученных результатов:**

Нам необходимо отследить, какими классифицирующими признаками изобилует тот или иной кластер, и на основе этого определить, по какому признаку будут охарактеризованы сами кластеры и как будут называться. Охарактеризуем наши кластеру по размеру/форме. Заметим, что 1-ый кластер (фиолетовый) отличается высоким показателем карата, то есть веса, и средними показателями глубины и ширины вершины относительно самой широкой точки, в чем превосходит другие кластеры, это **крупные** бриллианты. 2-ой кластер (красный) напротив имеет самый маленький вес, однако так же, как и 1-ый кластер, имеет средние показатели ширины и глубины, это **вытянутые** бриллианты, они менее дорогие, чем **крупные**. 3-ий кластер обладает высокими показателями веса и ширины и низким показателем глубины, это **широкие** бриллианты, они так же менее дорогие, чем **крупные**.

### **4. Описание кластеров с помощью графических средств, помогающих обосновать название кластеров:**

Визуализируем кластеры на диаграмме рассеивания, которая отображает первые два основных компонента на осях, используя функцию fivz_cluster(). Создадим 3 таких графика, так как кластеров у нас 3:

```{r}
fviz_cluster(kmeans3, data = cluster_df, method = "gap_stat", geom = "point", choose.vars = c('carat', 'depth'))
fviz_cluster(kmeans3, data = cluster_df, method = "gap_stat", geom = "point", choose.vars = c('depth', 'table'))
fviz_cluster(kmeans3, data = cluster_df, method = "gap_stat", geom = "point", choose.vars = c('table', 'carat'))
```

Первый график, на осях которого отложены вес и глубина на осях $Ox$ и $Oy$ соответственно, дает понять, что 1-ый кластер значительно превосходит 2 других по глубине, одновременно с тем имея высокий показатель веса. 3-ий кластер так же изобилует относительно увесистыми бриллиантами, однако, в сравнении с 1-ым кластером, в нем имеются бриллианты и с самым маленьким весом. Это дает нам понять, что в 1-ом кластере располагаются самые большие бриллианты, то есть, как мы обозначили ранее, **крупные**, по-совместительству самые дорогие, так как у цены и размера прямая зависимость. 2-ой же кластер изобилует наблюдениями, которые отличаются глубиной, однако не имеют такого веса, как большие бриллианты, что наталкивает нас на логичный вывод о том, что такие риллианты являются относительно длинными (в сравнении с другими), то есть **вытянутые**. 3-ий кластер включает в себя наблюдения, отличающиеся маленькой глубиной, но при этом большим весом, что дает сделать логичный вывод об относительной широте бриллианта (в сравнении с другими), это **широкие** бриллианты. Чтобы подтвердить выводы о кластерах и подтвердить какие-либо детали о них, можно посмотреть на них в других координатах при добавлении параметра ширины вершины относительно самой широкой точки. На втором и третьем графиках, где на одной из осей отложен этот параметр, видно, что большее количество наблюдений, которые могут "похвастаться" большой шириной вершины, содержится в 3-ем кластере, что так же является явным признаком того, что в кластере содержатся самые **широкие** бриллианты, так как ширина вершины считается относительно ширины самой широкой точки. Другими словами данный параметр характеризует то, какую площадь вершина составляет от самой широкой части бриллианта. Чем шире бриллиант, тем меньше разница между площадями вершины и широкой точки.

### **5. Вывод:**

Метод к-средних помог нам разделить нашу выборку на 3 кластера на основании размера бриллиантов и их формы, в которые входят **крупные**, **вытянутые** и **широкие** бриллианты. Кластеры местами пересекаются между собой. Визуализировав наши кластеры при помощи функции fivz_cluster(), мы убедились в закономерности форм бриллиантов в том или ином кластере. Проведя тесты на проверку гипотезы о равенстве средних, мы убедились, что она отвергается на любом уровне значимости.

## **Построение регрессионных моделей в кластерах (типологическая регрессия):**

### **1. Построение уравнений регрессии в кластерах:**

```{r}
df_clust_reg <- df[1:500,]
clusters <- as.data.frame(kmeans3$cluster, col.names = names('clust'))
df_clust_reg <- cbind(df_clust_reg, clusters)
```

Разобьем нашу выборку на 3 кластера:
```{r}
cluster1 <- df_clust_reg[kmeans3$cluster == 1,]
cluster1 <- cluster1[c('carat', 'depth', 'table', 'x', 'y', 'z', 'price')]

cluster2 <- df_clust_reg[kmeans3$cluster == 2,]
cluster2 <- cluster2[c('carat', 'depth', 'table', 'x', 'y', 'z', 'price')]

cluster3 <- df_clust_reg[kmeans3$cluster == 3,]
cluster3 <- cluster3[c('carat', 'depth', 'table', 'x', 'y', 'z', 'price')]
```

__1-ый__ кластер:
```{r}
regfit_fwd_c1 <- regsubsets(price ~., data = cluster1, intercept = TRUE,
                         method = 'forward')
regfit_fwd_sum <- summary(regfit_fwd_c1)
regfit_fwd_sum
names(regfit_fwd_sum)
regfit_fwd_sum$adjr2
```

Заметим, что пик объясняющей способности находится после добавления 4 переменных.
```{r}
plot(regfit_fwd_sum$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_fwd_sum$adjr2), regfit_fwd_sum$adjr2[which.max(regfit_fwd_sum$adjr2)],
       col = "red", cex = 2, pch = 20)
```

```{r}
lm_cluster_1 <- lm(data = cluster1, price ~ z + carat + x + table)
pander(summary(lm_cluster_1))
```

Незначимых переменных не наблюдается.
```{r}
summary(lm_cluster_1)
```
p-value < 0.05 уровня значимости.

На данном этапе необходимо проверить остатки на нормальность. Построим 3 графика:
```{r, warning = FALSE, message = FALSE}
res <- lm_cluster_1$residuals
dfres <- data.frame(cbind(res, freq(res)[,2]))

plot(seq(1, nrow(cluster1), 1), res, pch = 16, xlab = 'Номер наблюдения', ylab = 'Остаток')
abline(h = mean(res), col = 'red', lwd = 2)

ggplot(dfres, aes(x = res)) + 
  geom_histogram(color = "black", fill = "lightblue") + 
  ylab("Частота") +
  xlab("Остаток") +
  ggtitle('Гистограмма распределения частот остатков')

qqnorm(res, pch = 16, xlab = 'Теоретический квантиль', ylab = 'Выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res, col = 'red', lwd = 2)
```

Графики демонстрируют нам, что остатки распределены нормально.

Посмотрим на другие кластеры

__2-ой__ кластер:
```{r}
regfit_fwd_c2 <- regsubsets(price ~., data = cluster2, intercept = TRUE,
                         method = 'forward')
regfit_fwd_sum <- summary(regfit_fwd_c2)
regfit_fwd_sum
names(regfit_fwd_sum)
regfit_fwd_sum$adjr2
```

Заметим, что пик объясняющей способности находится после добавления 2 переменных.
```{r}
plot(regfit_fwd_sum$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_fwd_sum$adjr2), regfit_fwd_sum$adjr2[which.max(regfit_fwd_sum$adjr2)],
       col = "red", cex = 2, pch = 20)
```

```{r}
lm_cluster_2 <- lm(data = cluster2, price ~ carat + table)
pander(summary(lm_cluster_2))
```

Обе переменные незначимы, уберем одну.
```{r}
lm_cluster_2 <- lm(data = cluster2, price ~ carat)
pander(summary(lm_cluster_2))
```

```{r}
summary(lm_cluster_2)
```
p-value > 0.05 уровня значимости, но < 0.1.

На данном этапе необходимо проверить остатки на нормальность. Построим 3 графика:
```{r, warning = FALSE, message = FALSE}
res <- lm_cluster_2$residuals
dfres <- data.frame(cbind(res, freq(res)[,2]))

plot(seq(1, nrow(cluster2), 1), res, pch = 16, xlab = 'Номер наблюдения', ylab = 'Остаток')
abline(h = mean(res), col = 'red', lwd = 2)

ggplot(dfres, aes(x = res)) + 
  geom_histogram(color = "black", fill = "lightblue") + 
  ylab("Частота") +
  xlab("Остаток") +
  ggtitle('Гистограмма распределения частот остатков')

qqnorm(res, pch = 16, xlab = 'Теоретический квантиль', ylab = 'Выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res, col = 'red', lwd = 2)
```

__3-ий__ кластер:
```{r}
regfit_fwd_c3 <- regsubsets(price ~., data = cluster3, intercept = TRUE,
                         method = 'forward')
regfit_fwd_sum <- summary(regfit_fwd_c3)
regfit_fwd_sum
names(regfit_fwd_sum)
regfit_fwd_sum$adjr2
```

Заметим, что пик объясняющей способности находится после добавления 5 переменных.
```{r}
plot(regfit_fwd_sum$adjr2,xlab = "Количество переменных", 
     ylab = "Скорректированный коэффициент детерминации", type = "l")
points(which.max(regfit_fwd_sum$adjr2), regfit_fwd_sum$adjr2[which.max(regfit_fwd_sum$adjr2)],
       col = "red", cex = 2, pch = 20)
```

```{r}
lm_cluster_3 <- lm(data = cluster3, price ~ carat + z + y + x + depth)
pander(summary(lm_cluster_3))
```

Нам необходимо избавиться от незначимых переменных:
```{r}
lm_cluster_3 <- lm(data = cluster3, price ~ carat + z + x + depth)
pander(summary(lm_cluster_3))
```

```{r}
summary(lm_cluster_3)
```
p-value < 0.05 уровня значимости.

На данном этапе необходимо проверить остатки на нормальность. Построим 3 графика:
```{r, warning = FALSE, message = FALSE}
res <- lm_cluster_3$residuals
dfres <- data.frame(cbind(res, freq(res)[,2]))

plot(seq(1, nrow(cluster3), 1), res, pch = 16, xlab = 'Номер наблюдения', ylab = 'Остаток')
abline(h = mean(res), col = 'red', lwd = 2)

ggplot(dfres, aes(x = res)) + 
  geom_histogram(color = "black", fill = "lightblue") + 
  ylab("Частота") +
  xlab("Остаток") +
  ggtitle('Гистограмма распределения частот остатков')

qqnorm(res, pch = 16, xlab = 'Теоретический квантиль', ylab = 'Выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res, col = 'red', lwd = 2)
```

Графики демонстрируют нам, что остатки распределены нормально.

### **2. Сопоставление и интерпретация коэффициентов регрессии в кластерах с использованием коэффициентов эластичности:**



### **3. Сопоставление качества построенных моделей в кластерах и для всей совокупности объектов:__

Уравнение регрессии для всей совокупности:
$y(`price`) = 11981.21464  + 7865.10875 x(`carat`) - 126.12313x(`depth`) - 72.84498x(`table`) - 789.07653x(`x`) + 345.64540x(`y`) - 19.16809x(`z`)$

Коэффициент детерминации для всей совокупности объектов (скорректированный)  равен ~$0.84$, как мы выяснялм ранее. Заметим, что **1-ая** модель в кластерах обладает чуть большей объясняющей способностью, чего нельзя сказать про **2-ую** и **3-ую** модели. Однако стоит отметить, что разница между коэффициентом детерминации для всей совокупности объектов и **3-ей** моделью не высока. Несмотря на высокую объясняющую способность **1-ой** модели глобально разделение на кластеры не дало нам весомых преимуществ, поэтому, можно считать, что модель для всей совокупности объектов справляется с этим лучше. Данная модель так же имеет преимущество в скорости построения.

## **Дискриминантный анализ:**

### **Построение дискриминантных функций. Выводы о качестве модели:**

Присоединим вектор значений принадлежности к кластеру к основным данным:

```{r}
cl <- kmeans3$cluster
cluster_df <- as.data.frame(cbind(cluster_df, cl))
```

Разделим нашу выборку на две другие, а именно на обучающую и тестовую, которые будут составлять 2/3 и 1/3 доли от общей выборки соответственно.

```{r}
smpl_size <- floor(2/3 * nrow(cluster_df))

set.seed(222)
train_ind <- sample(seq_len(nrow(cluster_df)), size = smpl_size)

data.train <- as.data.frame(cluster_df[train_ind,])
data.unknown <- as.data.frame(cluster_df[-train_ind,])
```

Построение дискриминантных функций:

```{r}
str(data.train)
```

```{r, warning = FALSE, message = FALSE}
lda.fit <- lda(data.train[,-c(4,5)], data.train$cl)
lda.fit
```

Получаем 2 дискриминантные функции. Результат дает нам понять с какой вероятностью рядовое наблюдение может оказаться в том или ином кластере. Для 1, 2 и 3 кластеров это $0.62$, $0.23$ и $0.14$ соответственно. Данный тест показал, что $79$% общей дисперсии может быть объяснено при помощи LD1, а остальные 21%, в свою очередь, LD2. Дискриминантная функция довольно хорошо распределяет наблюдения на кластеры.

**Лямбда Уилкса** нам поможет сделать выводы о качестве модели:

```{r}
lda.pred <- predict(lda.fit, data.unknown[,-c(4, 5)])
names(lda.pred)
```

```{r}
ldam <- manova(as.matrix(data.unknown[,-c(4, 5)]) ~ lda.pred$class)

summary(ldam, test = "Wilks")
```

Результаты показывают, что лямбда Уилкса чуть больше табличного уровня значимости $0.05$, но меньше $0.1$. Будем считать разницу некритичной. Можно утверждать, что модель качественная.

### **2. Отнесение новых объектов к выделенным и описанным кластерам различными способами с использованием ДФ:**

На данном этапе нам необходимо определить, куда будет относиться новое наблюдение, а именно к какому из кластеров. Для осуществления задуманного нам поможет функция predict().

Проверим 3 наблюдения:

```{r}
print('1-ое наблюдение')
predict(lda.fit, newdata = data.frame(carat = 0.3, depth = 62.6, table = 55))
print('2-ое наблюдение')
predict(lda.fit, newdata = data.frame(carat = 0.24, depth = 65, table = 53))
print('3-ее наблюдение')
predict(lda.fit, newdata = data.frame(carat = 0.31, depth = 58.3, table = 63))
```

Все 3 наблюдения относятся к кластеру с самыми высокими значениями с примерно похожим вкладом для дискриминантных функций. Для **1-го** наблюдения $-10.8$ и $-4.1$, для **2-го** наблюдения $-11.5$ и $-0.3$, для **3-го** наблюдения $-9.5$ и $-15.1$.

### **3. Уточнение результатов классификации, выполненной с помощью к-средних, с помощью аппарата дискриминантного анализа:**

Построим график дискриминантной функции:

```{r}
plot(lda.fit)
```

Данный график дает понять, что распределение кластеров произошло лучшим образом.

Диаграмма рассеяния значений дискриминантных функций:

```{r}
plot(lda.pred$x[,1], lda.pred$x[,2]) # make a scatterplot
text(lda.pred$x[,1], lda.pred$x[,2], cl, cex = 0.7, pos = 4, col = "blue")
```

Диаграмма рассеяния демонстрирует схожесть значений между дискриминантными функциями.

Построим таблицу соответствия предсказанных классов исходным:

```{r}
lda.pred$class
table(lda.pred$class, data.unknown[,c("cl")])
summary(lda.pred$class)
```

Заметим, что в ходе данного анализа 7 объектов, принадлежащих на самом деле второму кластеру, были ошибочно отнесены к двум другим. 2 объекта из 3 кластера были ошибочно отнесены к первому кластеру. Первый кластер, в свою очередь, содержит все наблюдения из представленных. В целом, большинство наблюдений распределены по кластерам правильным образом.

### **4. Анализ классификационной матрицы. Вывод о качестве разбиения объектов на кластеры:**

Построим классификационную матрицу:

```{r}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.unknown[,c("cl")])
```

Общий уровень ошибок составляет $9.85$, что является довольно неплохим показателем. Можно сказать, что распределение наблюдений по кластерам происходит качественно.

### **5. Построение графика принадлежности тестовой и тренировочной выборок к кластерам по результатам проведенного анализа:**

Обратимся к **Методу Варда**.

Для обучающей выборки:

```{r}
data.train_new <- as.data.frame(cluster_df[train_ind,])
hclust_w1 <- hcut(data.train_new, k = 3, hc_metric = 'euclidian', hc_method = 'ward.D2') 
fviz_dend(hclust_w1, 
cex = 1, 
color_labels_by_k = TRUE, 
main = 'Дендрограмма (принцип Варда)', ylab = 'Расстояние') 
```

Разбиение обучающей выборки на 3 кластера демонстрирует хорошие результаты.

Для тестовой выборки:

```{r}
data.unknown_new <- as.data.frame(cluster_df[-train_ind,])
hclust_w2 <- hcut(data.unknown_new, k = 3, hc_metric = 'euclidian', hc_method = 'ward.D2') 
fviz_dend(hclust_w2, 
cex = 1, 
color_labels_by_k = TRUE, 
main = 'Дендрограмма (принцип Варда)', ylab = 'Расстояние') 
```

Распределение наблюдений из тестовой выборки по 3 кластерам так же успешно.

Визуализируем кластеры обеих выборок на диаграммах рассеивания в пространстве главных компонент, используя функцию fivz_cluster(). Создадим 2 графика:

Для обучающей выборки:

```{r}
complete_train <- data.train_new[complete.cases(data.train_new), ]
cluster_train <- scale(complete_train)
kmeans_train <- kmeans(cluster_train, centers = 3)
fviz_cluster(object = kmeans_train, data = data.train_new,
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры бриллиантов обучающей выборки в пространстве двух главных компонент')
```

Для тестовой выборки:

```{r}
complete_test <- data.unknown_new[complete.cases(data.unknown_new), ]
cluster_test <- scale(complete_test)
kmeans_test <- kmeans(cluster_test, centers = 3)
fviz_cluster(object = kmeans_test, data = data.unknown_new,
             ellipse.type = 'convex', geom = 'point',
             main = 'Кластеры бриллиантов тестовой выборки в пространстве двух главных компонент')
```

Как мы можем заметить, кластеры обеих выборок, как обучающей, так и тестовой, четко разделены друг от друга в пространсвте главных компонент. Тест Уилкса демонстрирует хорошее качество модели. Ошибка в разделении наблюдений приемлема.
